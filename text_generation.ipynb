{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvshyer/tensorflow2.0_learning/blob/master/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpewTAUzZ5uW",
        "colab_type": "text"
      },
      "source": [
        "### Text generation using a RNN with eager execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhjRynegZyld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuL3HVlYahed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the Shakespeare dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dDnHkjVaylX",
        "colab_type": "code",
        "outputId": "ce073b94-ccb7-49b1-b149-10825434e564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read the data\n",
        "# Read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYevK60ZbINk",
        "colab_type": "code",
        "outputId": "0f1a78ef-785a-434f-fb26-bfe334917e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppE5lApIbO4n",
        "colab_type": "code",
        "outputId": "cced713b-872a-4ce9-ee64-060cef6a1b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PURuiALbbZJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Lw2UCtbzD4",
        "colab_type": "code",
        "outputId": "6eedb2ed-2868-4c88-9ef3-c162d794c62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "  print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR4NZVblcQdo",
        "colab_type": "code",
        "outputId": "bd95dfe7-5439-4569-c3ab-11133d79fde5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sSWuCRtc9u9",
        "colab_type": "text"
      },
      "source": [
        "### Create training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpHgCP3Tc9Ex",
        "colab_type": "code",
        "outputId": "5a17910d-0449-4487-cc82-53a30c91641f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbwoBV5zcvX_",
        "colab_type": "code",
        "outputId": "9090f21f-378b-4847-900e-501fb5269096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3agvkWt0dygx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhV5jD9oeKuG",
        "colab_type": "code",
        "outputId": "6248e92a-ff6f-4cc8-a888-b7d387271e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fDIEsiwefN1",
        "colab_type": "code",
        "outputId": "712baf65-89a8-4ea5-aff1-36eb58455911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print('Step{:4d}'.format(i))\n",
        "  print(' input: {} ({:s})'.format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(' expected output: {} ({:s})'.format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step   0\n",
            " input: 18 ('F')\n",
            " expected output: 47 ('i')\n",
            "Step   1\n",
            " input: 47 ('i')\n",
            " expected output: 56 ('r')\n",
            "Step   2\n",
            " input: 56 ('r')\n",
            " expected output: 57 ('s')\n",
            "Step   3\n",
            " input: 57 ('s')\n",
            " expected output: 58 ('t')\n",
            "Step   4\n",
            " input: 58 ('t')\n",
            " expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJrMJM0ufUWn",
        "colab_type": "code",
        "outputId": "06c8585b-1676-4dcb-f42b-8bf17f1aa3ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create training batches\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3FmdjdhETo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpD3T0PXhNgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1npaR3AiBik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "      rnn(rnn_units,\n",
        "          return_sequences=True,\n",
        "          recurrent_initializer='glorot_uniform',\n",
        "          stateful=True),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPk26zvUihKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim = embedding_dim,\n",
        "  rnn_units = rnn_units,\n",
        "  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZjtxunuizjA",
        "colab_type": "code",
        "outputId": "23cc31ad-9cb2-4806-fc7a-37c0c2475c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psa_ii-VjH8j",
        "colab_type": "code",
        "outputId": "97e09713-4186-4712-f4ba-fa50345aac54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOf4p105jsI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try it for the 1st example in the batch\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtEtNF-oj8R8",
        "colab_type": "code",
        "outputId": "3d2d7ca8-8fe7-44aa-9728-86bb87c8edb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  6, 21,  4,  2, 38, 30, 24, 55, 27, 37, 20, 57, 32, 55, 35, 29,\n",
              "       19, 22, 14, 52, 54, 23, 55, 37, 57, 58,  3,  6, 54, 34, 25, 10, 34,\n",
              "       55, 24, 12, 49, 21, 51, 45, 55, 35,  6, 17, 20, 25,  8,  4,  2, 61,\n",
              "       48,  1, 23,  6, 30,  9, 10, 43, 18, 27, 54, 28, 26, 44, 13, 21, 60,\n",
              "       30,  9, 35, 54, 17, 31, 33, 11, 47, 12, 22, 14, 18, 46, 44, 31, 46,\n",
              "       48, 32, 46, 47, 10, 46, 56,  2, 36, 34, 33,  7, 30, 29, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmbJnwTbj_dX",
        "colab_type": "code",
        "outputId": "5bfbcddc-6d09-416f-e397-021a1176fdf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Decode\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \",\\n'Thus saith the duke, thus hath the duke inferr'd;'\\nBut nothing spake in warrant from himself.\\nWhe\"\n",
            "\n",
            "Next Char Predictions: \n",
            " '$,I&!ZRLqOYHsTqWQGJBnpKqYst$,pVM:VqL?kImgqW,EHM.&!wj K,R3:eFOpPNfAIvR3WpESU;i?JBFhfShjThi:hr!XVU-RQP'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvNK_6PRkTiI",
        "colab_type": "code",
        "outputId": "e6d4df28-e16a-4621-e933-819ac037d286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Train the model\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.173156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWjALBgQlHBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "  optimizer = tf.train.AdamOptimizer(),\n",
        "  loss = loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxN7pFgblO2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the ckp file\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath = checkpoint_prefix,\n",
        "  save_weights_only = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCgqM4WMlpaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyiFTS3jlreU",
        "colab_type": "code",
        "outputId": "46f9c7fb-afa0-4b16-b511-6c5cc7489899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        }
      },
      "source": [
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "173/174 [============================>.] - ETA: 0s - loss: 2.6856WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
            "174/174 [==============================] - 27s 157ms/step - loss: 2.6824\n",
            "Epoch 2/30\n",
            "174/174 [==============================] - 25s 143ms/step - loss: 1.9332\n",
            "Epoch 3/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 1.6708\n",
            "Epoch 4/30\n",
            "174/174 [==============================] - 25s 143ms/step - loss: 1.5292\n",
            "Epoch 5/30\n",
            "174/174 [==============================] - 25s 143ms/step - loss: 1.4457\n",
            "Epoch 6/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 1.3894\n",
            "Epoch 7/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 1.3453\n",
            "Epoch 8/30\n",
            "174/174 [==============================] - 25s 146ms/step - loss: 1.3092\n",
            "Epoch 9/30\n",
            "174/174 [==============================] - 25s 142ms/step - loss: 1.2736\n",
            "Epoch 10/30\n",
            "174/174 [==============================] - 25s 143ms/step - loss: 1.2455\n",
            "Epoch 11/30\n",
            "174/174 [==============================] - 25s 145ms/step - loss: 1.2137\n",
            "Epoch 12/30\n",
            "174/174 [==============================] - 25s 145ms/step - loss: 1.1825\n",
            "Epoch 13/30\n",
            "174/174 [==============================] - 25s 145ms/step - loss: 1.1528\n",
            "Epoch 14/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 1.1227\n",
            "Epoch 15/30\n",
            "174/174 [==============================] - 25s 143ms/step - loss: 1.0888\n",
            "Epoch 16/30\n",
            "174/174 [==============================] - 25s 143ms/step - loss: 1.0562\n",
            "Epoch 17/30\n",
            "174/174 [==============================] - 25s 146ms/step - loss: 1.0218\n",
            "Epoch 18/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 0.9895\n",
            "Epoch 19/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 0.9564\n",
            "Epoch 20/30\n",
            "174/174 [==============================] - 25s 145ms/step - loss: 0.9224\n",
            "Epoch 21/30\n",
            "174/174 [==============================] - 25s 146ms/step - loss: 0.8945\n",
            "Epoch 22/30\n",
            "174/174 [==============================] - 25s 146ms/step - loss: 0.8656\n",
            "Epoch 23/30\n",
            "174/174 [==============================] - 25s 146ms/step - loss: 0.8401\n",
            "Epoch 24/30\n",
            "174/174 [==============================] - 25s 144ms/step - loss: 0.8163\n",
            "Epoch 25/30\n",
            "174/174 [==============================] - 25s 146ms/step - loss: 0.7946\n",
            "Epoch 26/30\n",
            "174/174 [==============================] - 26s 147ms/step - loss: 0.7746\n",
            "Epoch 27/30\n",
            "174/174 [==============================] - 26s 148ms/step - loss: 0.7603\n",
            "Epoch 28/30\n",
            "174/174 [==============================] - 26s 147ms/step - loss: 0.7441\n",
            "Epoch 29/30\n",
            "174/174 [==============================] - 26s 148ms/step - loss: 0.7333\n",
            "Epoch 30/30\n",
            "174/174 [==============================] - 25s 145ms/step - loss: 0.7232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvD2HlxJl0by",
        "colab_type": "code",
        "outputId": "8056f9cd-0ce0-48b8-b236-d54486bcbfa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Restore the latest checkpoint\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-1su5ECng4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk-TXd29oJbL",
        "colab_type": "code",
        "outputId": "1e1c1d44-9a3e-418e-eca7-46b5d0e037f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvfbxbGoN2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTQCL8ZGpnms",
        "colab_type": "code",
        "outputId": "ab83774c-451c-4cf1-fc34-93188a499cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"JOHN: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JOHN: I have print\n",
            "like to ins, now fear up justice:\n",
            "These eyes that he gives his disdain I will practise\n",
            "this word. Thursday war.\n",
            "Here in this poor fair without disposition.\n",
            "\n",
            "FLORIZEL:\n",
            "What, would he swear,\n",
            "Deep as in a name, shook; what\n",
            "Master lives: when the senate has gatest\n",
            "To rave the instruct them on.\n",
            "\n",
            "JULIET:\n",
            "Good even too, your parties,--as the east against the Tower:\n",
            "My Lady Boltague, I'll make it my mead more. or I will have King Henry's art,\n",
            "Thou didst precedent when he wailing eyes\n",
            "O court, for this great loss,\n",
            "That Harry Duke of Norfolk, Thords, when it is defence!\n",
            "Now are you of the parliament-house!\n",
            "\n",
            "MENENIUS:\n",
            "Sir Richard! the nature of this haste.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Who was the lark, you that wit it. Will't the French.\n",
            "I talk of men, thy father gave the prisoner. First, would have held him care.\n",
            "\n",
            "ROMEO:\n",
            "A hunbraved and ebbraring life in Padua\n",
            "Of March's window name; I'll plant Patiencl;\n",
            "I not receive the harmless shades of night\n",
            "I tell you home to shade, blame that the eastern\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxdvsM13pucx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Customized Training\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim = embedding_dim,\n",
        "  rnn_units = rnn_units,\n",
        "  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00iKULKt7Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge23dZ5UuDzG",
        "colab_type": "code",
        "outputId": "d65387a7-ecdb-4668-d08a-75e6a69481e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "  \n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # feeding the hidden state back into the model\n",
        "      # This is the interesting step\n",
        "      predictions = model(inp)\n",
        "      loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "    \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    \n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {:4f}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "      \n",
        "  # saving (ckpt) the model every 5 epochs\n",
        "  if (epoch+1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch = epoch))\n",
        "    \n",
        "  print('Epoch {} Loss {:4f}'.format(epoch+1, loss))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  \n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1 Batch 0 Loss 4.173581\n",
            "Epoch 1 Batch 100 Loss 2.328911\n",
            "Epoch 1 Loss 2.131164\n",
            "Time taken for 1 epoch 26.56835627555847 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRzri7gtvPav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}