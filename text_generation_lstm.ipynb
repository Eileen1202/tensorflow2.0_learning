{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvshyer/tensorflow2.0_learning/blob/master/text_generation_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-WBVi9AvK5N",
        "colab_type": "code",
        "outputId": "d2979360-d90c-4378-fd58-3069386a7616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.7)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4orP3oGlPG5",
        "colab_type": "code",
        "outputId": "6823cb16-8d9f-4778-b05b-144f84d8dd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n",
            "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n",
            "matplotlib 3.0.3\n",
            "numpy 1.16.2\n",
            "pandas 0.24.2\n",
            "sklearn 0.21.2\n",
            "tensorflow 2.0.0-alpha0\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvZnT0otlaRe",
        "colab_type": "code",
        "outputId": "79703ae7-4474-4292-fd31-7b30b16944d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49juUEYlu2l",
        "colab_type": "code",
        "outputId": "2260809e-5718-48a1-a440-284bf55d3cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "text = open(path_to_file, 'r').read()\n",
        "\n",
        "print(len(text))\n",
        "print(text[0:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCLVbuL0meRY",
        "colab_type": "code",
        "outputId": "a789b832-351e-44b9-b479-e467a7770815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 1. generate vocab\n",
        "# 2. build mapping char->id\n",
        "# 3. data -> id_data\n",
        "# 4. abcd -> bcd<eos>\n",
        "\n",
        "# 将字符串转为set\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "print(len(vocab))\n",
        "print(vocab)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qI1Owq2m-Xw",
        "colab_type": "code",
        "outputId": "f50a4d23-73ac-40b0-c7d5-10a170a19a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
        "print(char2idx)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVgbM2kunIm0",
        "colab_type": "code",
        "outputId": "ba01f8a4-0e89-4478-a1eb-8611e2f8e631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "idx2char = np.array(vocab)\n",
        "print(idx2char)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7mVevcBnR6k",
        "colab_type": "code",
        "outputId": "d7589885-0660-4ec3-cb85-0e805558e261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(text_as_int[0:10])\n",
        "print(text[0:10])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47]\n",
            "First Citi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0OCkcBdna4i",
        "colab_type": "code",
        "outputId": "22dc14d6-e81d-4527-e52f-892194e21b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "def split_input_target(id_text):\n",
        "    # abcde -> abcd, bcde\n",
        "    return id_text[0:-1], id_text[1:]\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "seq_length = 100\n",
        "seq_dataset = char_dataset.batch(seq_length + 1, \n",
        "                                 drop_remainder = True)\n",
        "for ch_id in char_dataset.take(2):\n",
        "    print(ch_id, idx2char[ch_id.numpy()])\n",
        "    \n",
        "for seq_id in seq_dataset.take(2):\n",
        "    print(seq_id)\n",
        "    print(repr(''.join(idx2char[seq_id.numpy()])))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(18, shape=(), dtype=int64) F\n",
            "tf.Tensor(47, shape=(), dtype=int64) i\n",
            "tf.Tensor(\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59  1], shape=(101,), dtype=int64)\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "tf.Tensor(\n",
            "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
            " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
            " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
            "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
            " 63 53 59  1 49], shape=(101,), dtype=int64)\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx7hGFc0npGf",
        "colab_type": "code",
        "outputId": "b0e47d78-42d6-46f6-812d-7a07eaabc73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "seq_dataset = seq_dataset.map(split_input_target)\n",
        "\n",
        "for item_input, item_output in seq_dataset.take(2):\n",
        "    print(item_input.numpy())\n",
        "    print(item_output.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59]\n",
            "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
            " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
            " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
            "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
            " 37 53 59  1]\n",
            "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
            " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
            " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
            "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
            " 63 53 59  1]\n",
            "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
            " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
            " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
            "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
            " 53 59  1 49]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39f4ThGoiDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
        "    batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7tf_d6spDY6",
        "colab_type": "code",
        "outputId": "2b24868b-bf41-457b-e3dd-e0d5a630632b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                               batch_input_shape = [batch_size, None]),\n",
        "        keras.layers.LSTM(units = rnn_units,\n",
        "                          stateful = True,\n",
        "                          recurrent_initializer='glorot_uniform',\n",
        "                          return_sequences = True),\n",
        "        keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units, \n",
        "    batch_size = batch_size)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0719 07:19:33.528047 139669300365184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f06c73abf98>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "unified_lstm (UnifiedLSTM)   (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXrCpxcbt62u",
        "colab_type": "code",
        "outputId": "d9cea46c-3577-45d1-c9a6-2f00418c977b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAeFqSwiuOtu",
        "colab_type": "code",
        "outputId": "bf14b0df-de2a-47b1-a67f-e3022eac102c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# random sampling. \n",
        "# greedy策略; random策略.\n",
        "sample_indices = tf.random.categorical(\n",
        "    logits = example_batch_predictions[0], num_samples = 1)\n",
        "print(sample_indices)\n",
        "# (100, 65) -> (100, 1)\n",
        "sample_indices = tf.squeeze(sample_indices, axis = -1)\n",
        "print(sample_indices)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[10]\n",
            " [17]\n",
            " [17]\n",
            " [53]\n",
            " [ 4]\n",
            " [42]\n",
            " [44]\n",
            " [64]\n",
            " [ 3]\n",
            " [38]\n",
            " [42]\n",
            " [ 8]\n",
            " [22]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [49]\n",
            " [27]\n",
            " [39]\n",
            " [38]\n",
            " [46]\n",
            " [24]\n",
            " [ 4]\n",
            " [62]\n",
            " [20]\n",
            " [64]\n",
            " [44]\n",
            " [12]\n",
            " [15]\n",
            " [61]\n",
            " [13]\n",
            " [ 1]\n",
            " [61]\n",
            " [46]\n",
            " [23]\n",
            " [51]\n",
            " [10]\n",
            " [12]\n",
            " [49]\n",
            " [20]\n",
            " [20]\n",
            " [19]\n",
            " [50]\n",
            " [15]\n",
            " [26]\n",
            " [16]\n",
            " [57]\n",
            " [38]\n",
            " [42]\n",
            " [28]\n",
            " [21]\n",
            " [48]\n",
            " [38]\n",
            " [56]\n",
            " [19]\n",
            " [15]\n",
            " [64]\n",
            " [23]\n",
            " [10]\n",
            " [13]\n",
            " [55]\n",
            " [34]\n",
            " [45]\n",
            " [24]\n",
            " [ 4]\n",
            " [32]\n",
            " [49]\n",
            " [50]\n",
            " [13]\n",
            " [14]\n",
            " [60]\n",
            " [ 4]\n",
            " [12]\n",
            " [ 4]\n",
            " [51]\n",
            " [63]\n",
            " [64]\n",
            " [60]\n",
            " [33]\n",
            " [42]\n",
            " [12]\n",
            " [26]\n",
            " [48]\n",
            " [33]\n",
            " [ 3]\n",
            " [16]\n",
            " [31]\n",
            " [49]\n",
            " [37]\n",
            " [11]\n",
            " [ 9]\n",
            " [ 9]\n",
            " [59]\n",
            " [40]\n",
            " [ 9]\n",
            " [31]\n",
            " [36]\n",
            " [43]\n",
            " [53]\n",
            " [19]\n",
            " [ 7]], shape=(100, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[10 17 17 53  4 42 44 64  3 38 42  8 22  9  1 49 27 39 38 46 24  4 62 20\n",
            " 64 44 12 15 61 13  1 61 46 23 51 10 12 49 20 20 19 50 15 26 16 57 38 42\n",
            " 28 21 48 38 56 19 15 64 23 10 13 55 34 45 24  4 32 49 50 13 14 60  4 12\n",
            "  4 51 63 64 60 33 42 12 26 48 33  3 16 31 49 37 11  9  9 59 40  9 31 36\n",
            " 43 53 19  7], shape=(100,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWx2ueIYybuC",
        "colab_type": "code",
        "outputId": "510fc7d8-36c7-418f-e2d8-5d1551a4fa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Output: \", repr(\"\".join(idx2char[target_example_batch[0]])))\n",
        "print()\n",
        "print(\"Predictions: \", repr(\"\".join(idx2char[sample_indices])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  \"ne! all men call thee fickle:\\nIf thou art fickle, what dost thou with him.\\nThat is renown'd for fait\"\n",
            "\n",
            "Output:  \"e! all men call thee fickle:\\nIf thou art fickle, what dost thou with him.\\nThat is renown'd for faith\"\n",
            "\n",
            "Predictions:  ':EEo&dfz$Zd.J3 kOaZhL&xHzf?CwA whKm:?kHHGlCNDsZdPIjZrGCzK:AqVgL&TklABv&?&myzvUd?NjU$DSkY;33ub3SXeoG-'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHALQFszCg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy(\n",
        "        labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSIHZOO-3az3",
        "colab_type": "code",
        "outputId": "62269faa-6143-4500-ff76-b6d571a77a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)\n",
        "\n",
        "example_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(example_loss.shape)\n",
        "print(example_loss.numpy().mean())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100)\n",
            "4.173719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7VFMrb93mEl",
        "colab_type": "code",
        "outputId": "8b03cc92-72a9-4761-8963-fe6c3ec05ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "output_dir = \"./text_generation_checkpoints\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    \n",
        "checkpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True)\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "history = model.fit(seq_dataset, epochs = epochs,\n",
        "                    callbacks = [checkpoint_callback])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 1.6834\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 1.5268\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 1.4338\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 1.3679\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 1.3144\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 1.2674\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 1.2224\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 1.1781\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 1.1353\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 1.0973\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 1.0618\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 1.0229\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.9823\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.9481\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.9177\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.8856\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.8581\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.8289\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.8004\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.7768\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.7510\n",
            "Epoch 22/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.7218\n",
            "Epoch 23/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.6962\n",
            "Epoch 24/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.6754\n",
            "Epoch 25/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.6567\n",
            "Epoch 26/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.6398\n",
            "Epoch 27/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.6231\n",
            "Epoch 28/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.6040\n",
            "Epoch 29/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.5883\n",
            "Epoch 30/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.5723\n",
            "Epoch 31/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.5588\n",
            "Epoch 32/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.5409\n",
            "Epoch 33/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.5283\n",
            "Epoch 34/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.5147\n",
            "Epoch 35/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.5058\n",
            "Epoch 36/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4963\n",
            "Epoch 37/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4882\n",
            "Epoch 38/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4808\n",
            "Epoch 39/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4687\n",
            "Epoch 40/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.4579\n",
            "Epoch 41/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4478\n",
            "Epoch 42/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4393\n",
            "Epoch 43/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.4315\n",
            "Epoch 44/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4224\n",
            "Epoch 45/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4133\n",
            "Epoch 46/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4074\n",
            "Epoch 47/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.4003\n",
            "Epoch 48/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3919\n",
            "Epoch 49/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3884\n",
            "Epoch 50/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3823\n",
            "Epoch 51/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.3779\n",
            "Epoch 52/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3746\n",
            "Epoch 53/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3679\n",
            "Epoch 54/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3669\n",
            "Epoch 55/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3602\n",
            "Epoch 56/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3554\n",
            "Epoch 57/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3490\n",
            "Epoch 58/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.3445\n",
            "Epoch 59/100\n",
            "172/172 [==============================] - 30s 174ms/step - loss: 0.3427\n",
            "Epoch 60/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3420\n",
            "Epoch 61/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3387\n",
            "Epoch 62/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3358\n",
            "Epoch 63/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3361\n",
            "Epoch 64/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3311\n",
            "Epoch 65/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3304\n",
            "Epoch 66/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3280\n",
            "Epoch 67/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3247\n",
            "Epoch 68/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3221\n",
            "Epoch 69/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3189\n",
            "Epoch 70/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3154\n",
            "Epoch 71/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3143\n",
            "Epoch 72/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3144\n",
            "Epoch 73/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3098\n",
            "Epoch 74/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.3089\n",
            "Epoch 75/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.3043\n",
            "Epoch 76/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2988\n",
            "Epoch 77/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2968\n",
            "Epoch 78/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2924\n",
            "Epoch 79/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2926\n",
            "Epoch 80/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2895\n",
            "Epoch 81/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2905\n",
            "Epoch 82/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2858\n",
            "Epoch 83/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2861\n",
            "Epoch 84/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2851\n",
            "Epoch 85/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2836\n",
            "Epoch 86/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2854\n",
            "Epoch 87/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2863\n",
            "Epoch 88/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2852\n",
            "Epoch 89/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2854\n",
            "Epoch 90/100\n",
            "172/172 [==============================] - 30s 175ms/step - loss: 0.2820\n",
            "Epoch 91/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2762\n",
            "Epoch 92/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2762\n",
            "Epoch 93/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2767\n",
            "Epoch 94/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2748\n",
            "Epoch 95/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2695\n",
            "Epoch 96/100\n",
            "172/172 [==============================] - 30s 172ms/step - loss: 0.2675\n",
            "Epoch 97/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2628\n",
            "Epoch 98/100\n",
            "172/172 [==============================] - 30s 172ms/step - loss: 0.2614\n",
            "Epoch 99/100\n",
            "172/172 [==============================] - 30s 172ms/step - loss: 0.2673\n",
            "Epoch 100/100\n",
            "172/172 [==============================] - 30s 173ms/step - loss: 0.2675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NY23Hc6Lom",
        "colab_type": "code",
        "outputId": "a2ee4ce6-a729-404b-fe4e-2ea034c4f8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(output_dir)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./text_generation_checkpoints/ckpt_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6xuSD-UHUNB",
        "colab_type": "code",
        "outputId": "3d7609a0-d65d-4c59-c710-62869750587f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "model2 = build_model(vocab_size,\n",
        "                     embedding_dim,\n",
        "                     rnn_units,\n",
        "                     batch_size = 1)\n",
        "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
        "model2.build(tf.TensorShape([1, None]))\n",
        "# start ch sequence A,\n",
        "# A -> model -> b\n",
        "# A.append(b) -> B\n",
        "# B(Ab) -> model ->c\n",
        "# B.append(c) -> C\n",
        "# C(Abc) -> model -> ..\n",
        "model2.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0719 08:14:10.494503 139669300365184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f06c2274160>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "unified_lstm_1 (UnifiedLSTM) (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNEPwXM7HzxV",
        "colab_type": "code",
        "outputId": "e53bdfc6-9bde-4d60-ef03-da2222fb9de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "def generate_text(model, start_string, num_generate=1000):\n",
        "    input_eval = [char2idx[ch] for ch in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    model.reset_states()\n",
        "    \n",
        "    # temperature >1, random\n",
        "    # temperature <1, greedy\n",
        "    temperature = 0.5\n",
        "    \n",
        "    for _ in range(num_generate):\n",
        "        # 1. model inference -> predictions\n",
        "        # predictions: [batch_size, input_eval_len, vocab_size]\n",
        "        predictions = model(input_eval)\n",
        "        # predictions: logits -> softmax -> prob\n",
        "        # softmax: e^xi\n",
        "        # eg:4,2 e^4/(e^4+e^2)=0.88, e^2/(e^4+e^2)=0.12\n",
        "        # eg:2,1 e^2(e^2 + e)=0.73, e/(e^2+e)=0.27\n",
        "        predictions = predictions / temperature\n",
        "        # predictions: [input_eval_len, vocab_size]\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # predictions: [input_eval_len, 1]\n",
        "        predicted_id = tf.random.categorical(\n",
        "            predictions, num_samples = 1)[-1, 0].numpy()\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        # s, x -> rnn -> s', y\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "new_text = generate_text(model2, \"All\")\n",
        "        # 2. sample -> ch ->text_generated.\n",
        "        # 3. undate input_eval\n",
        "print(new_text)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allk of such a coldurers;\n",
            "And cannot lie with thee as you take in your chance.\n",
            "\n",
            "ISABELLA:\n",
            "O you beast!\n",
            "O faith, a great matter and so great a weighty cause\n",
            "Of love between your highness or the place,\n",
            "That brought them in; so hot, they will be oaths\n",
            "With pure poiling where it is,\n",
            "And leave your brothers to report this valour\n",
            "content of our officers.\n",
            "\n",
            "Provost:\n",
            "Here is the head of tears.\n",
            "\n",
            "Clown:\n",
            "He shall be there no man: some noiser is true.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I pray you, sir, but young King Licion\n",
            "Clarence doth his eyes thereof that faced repend;\n",
            "And I him in them back again,\n",
            "Not this metal, of the house of York\n",
            "Is a strange brooch in this appetite,\n",
            "Hasting to the sou and so many lives\n",
            "Unto the soldiers to the judge:\n",
            "If your doubler and courage most strange\n",
            "In their pilot of his offence!\n",
            "What strange from such cheerly shall there is slain,\n",
            "And counsel may my knight, and earth that slay these fellows?\n",
            "\n",
            "LADY CAPULET:\n",
            "Hark, here long in hope,\n",
            "But I flouse thee so much with the boty?\n",
            "\n",
            "PAULINA:\n",
            "An\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bfqQ8tQJL0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}