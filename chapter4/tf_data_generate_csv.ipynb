{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_data_generate_csv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvshyer/tensorflow2.0_learning/blob/master/tf_data_generate_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2c4YF9mUUIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "009d9601-8df1-46de-924b-13a295edfe62"
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9MB 487kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 28.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.16.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 38.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: google-pasta, tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed google-pasta-0.1.7 tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ezK57pS6ZSF",
        "colab_type": "code",
        "outputId": "ba890d00-57de-440e-a2ac-9d85cd95d486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n",
            "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
            "matplotlib 3.0.3\n",
            "numpy 1.16.4\n",
            "pandas 0.24.2\n",
            "sklearn 0.21.2\n",
            "tensorflow 2.0.0-alpha0\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd13Iimm6j7Q",
        "colab_type": "code",
        "outputId": "7528ed6d-1c94-4ff7-8d3d-61a3a417649c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "print(housing.DESCR)\n",
        "print(housing.data.shape)\n",
        "print(housing.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "I0604 02:30:07.536153 140271430248320 california_housing.py:114] Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block\n",
            "        - HouseAge      median house age in block\n",
            "        - AveRooms      average number of rooms\n",
            "        - AveBedrms     average number of bedrooms\n",
            "        - Population    block population\n",
            "        - AveOccup      average house occupancy\n",
            "        - Latitude      house block latitude\n",
            "        - Longitude     house block longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "http://lib.stat.cmu.edu/datasets/\n",
            "\n",
            "The target variable is the median house value for California districts.\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n",
            "(20640, 8)\n",
            "(20640,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy2zeFXF7O6V",
        "colab_type": "code",
        "outputId": "7a3042dd-35b2-49e9-dbbf-edade73e9b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# test_size默认为0.25\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state = 7, test_size = 0.25)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_all, y_train_all, random_state = 11)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11610, 8) (11610,)\n",
            "(3870, 8) (3870,)\n",
            "(5160, 8) (5160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvaW64pQ7418",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsIxnZqD9o3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dir = \"generate_csv\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    \n",
        "def save_to_csv(output_dir, data, name_prefix, \n",
        "                header=None, n_parts=10):\n",
        "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
        "    filenames = []\n",
        "    \n",
        "    for file_idx, row_indices in enumerate(\n",
        "        np.array_split(np.arange(len(data)), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filenames.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header + \"\\n\")\n",
        "            for row_index in row_indices:\n",
        "                f.write(\",\".join(\n",
        "                    [repr(col) for col in data[row_index]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filenames\n",
        "\n",
        "train_data = np.c_[x_train_scaled, y_train]\n",
        "valid_data = np.c_[x_valid_scaled, y_valid]\n",
        "test_data = np.c_[x_test_scaled, y_test]\n",
        "\n",
        "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
        "header_str = \",\".join(header_cols)\n",
        "\n",
        "train_filenames = save_to_csv(output_dir, train_data, \"train\", \n",
        "                              header_str, n_parts=20)\n",
        "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\",\n",
        "                              header_str, n_parts=10)\n",
        "test_filenames = save_to_csv(output_dir, test_data, \"test\",\n",
        "                              header_str, n_parts=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2qwBsnpSuaN",
        "colab_type": "code",
        "outputId": "5d44cad4-c65e-411f-c4b1-918a6c11ba2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generate_csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ZqD-FOUzaq",
        "colab_type": "code",
        "outputId": "aa816a4d-783b-487d-e417-5038910c3d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!ls generate_csv/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_00.csv  test_08.csv   train_06.csv  train_14.csv  valid_02.csv\n",
            "test_01.csv  test_09.csv   train_07.csv  train_15.csv  valid_03.csv\n",
            "test_02.csv  train_00.csv  train_08.csv  train_16.csv  valid_04.csv\n",
            "test_03.csv  train_01.csv  train_09.csv  train_17.csv  valid_05.csv\n",
            "test_04.csv  train_02.csv  train_10.csv  train_18.csv  valid_06.csv\n",
            "test_05.csv  train_03.csv  train_11.csv  train_19.csv  valid_07.csv\n",
            "test_06.csv  train_04.csv  train_12.csv  valid_00.csv  valid_08.csv\n",
            "test_07.csv  train_05.csv  train_13.csv  valid_01.csv  valid_09.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NnZMrb8U6Lf",
        "colab_type": "code",
        "outputId": "9718a75f-6480-4bce-94c2-b413ad1b5e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "import pprint\n",
        "\n",
        "print(\"train filenames:\")\n",
        "pprint.pprint(train_filenames)\n",
        "print(\"valid filenames:\")\n",
        "pprint.pprint(valid_filenames)\n",
        "print(\"test filenames:\")\n",
        "pprint.pprint(test_filenames)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train filenames:\n",
            "['generate_csv/train_00.csv',\n",
            " 'generate_csv/train_01.csv',\n",
            " 'generate_csv/train_02.csv',\n",
            " 'generate_csv/train_03.csv',\n",
            " 'generate_csv/train_04.csv',\n",
            " 'generate_csv/train_05.csv',\n",
            " 'generate_csv/train_06.csv',\n",
            " 'generate_csv/train_07.csv',\n",
            " 'generate_csv/train_08.csv',\n",
            " 'generate_csv/train_09.csv',\n",
            " 'generate_csv/train_10.csv',\n",
            " 'generate_csv/train_11.csv',\n",
            " 'generate_csv/train_12.csv',\n",
            " 'generate_csv/train_13.csv',\n",
            " 'generate_csv/train_14.csv',\n",
            " 'generate_csv/train_15.csv',\n",
            " 'generate_csv/train_16.csv',\n",
            " 'generate_csv/train_17.csv',\n",
            " 'generate_csv/train_18.csv',\n",
            " 'generate_csv/train_19.csv']\n",
            "valid filenames:\n",
            "['generate_csv/valid_00.csv',\n",
            " 'generate_csv/valid_01.csv',\n",
            " 'generate_csv/valid_02.csv',\n",
            " 'generate_csv/valid_03.csv',\n",
            " 'generate_csv/valid_04.csv',\n",
            " 'generate_csv/valid_05.csv',\n",
            " 'generate_csv/valid_06.csv',\n",
            " 'generate_csv/valid_07.csv',\n",
            " 'generate_csv/valid_08.csv',\n",
            " 'generate_csv/valid_09.csv']\n",
            "test filenames:\n",
            "['generate_csv/test_00.csv',\n",
            " 'generate_csv/test_01.csv',\n",
            " 'generate_csv/test_02.csv',\n",
            " 'generate_csv/test_03.csv',\n",
            " 'generate_csv/test_04.csv',\n",
            " 'generate_csv/test_05.csv',\n",
            " 'generate_csv/test_06.csv',\n",
            " 'generate_csv/test_07.csv',\n",
            " 'generate_csv/test_08.csv',\n",
            " 'generate_csv/test_09.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo0vjpLXVccj",
        "colab_type": "code",
        "outputId": "eb638b1f-d709-4848-80fb-2764160ff98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# 读取csv 生成dataset\n",
        "# 1. filename -> dataset\n",
        "# 2. read file -> dataset -> datasets -> merge\n",
        "# 3. parse csv\n",
        "\n",
        "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
        "for filename in filename_dataset:\n",
        "    print(filename)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'generate_csv/train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_08.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_06.csv', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssAa4rsOV03A",
        "colab_type": "code",
        "outputId": "ed2b046b-4b35-4a22-973b-2288e9064012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "n_readers = 5\n",
        "dataset = filename_dataset.interleave(\n",
        "    # skip(1) 跳过header行\n",
        "    lambda filename: tf.data.TextLineDataset(filename).skip(1), # 按行读取文本 生成dataset\n",
        "    cycle_length = n_readers\n",
        ")\n",
        "\n",
        "for line in dataset.take(15):\n",
        "    print(line.numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'0.09734603446040174,0.7527628439249472,-0.20218964416999152,-0.1954700015215477,-0.4060513603629498,0.006785531677655949,-0.813715166526018,0.656614793197258,1.119'\n",
            "b'0.04326300977263167,-1.0895425985107923,-0.38878716774583305,-0.10789864528874438,-0.6818663605100649,-0.0723871014747467,-0.8883662012710817,0.8213992340186296,1.426'\n",
            "b'0.6303435674178064,1.874166156711919,-0.06713214279531016,-0.12543366804152128,-0.19737553788322462,-0.022722631725889016,-0.692407235065288,0.7265233438487496,2.419'\n",
            "b'-0.8219588176953616,1.874166156711919,0.18212349433218608,-0.03170019246279883,-0.6011178900722581,-0.14337494105109344,1.0852205298015787,-0.8613994495208361,1.054'\n",
            "b'0.4853051504718848,-0.8492418886278699,-0.06530126513877861,-0.023379656040017353,1.4974350551260218,-0.07790657783453239,-0.9023632702857819,0.7814514907892068,2.956'\n",
            "b'-1.4803330571456954,-0.6890414153725881,-0.35624704887282904,-0.1725588908792445,-0.8215884329530113,-0.1382309124854157,1.9157132913404298,-1.0211904224385344,0.928'\n",
            "b'-0.7543417158936074,-0.9293421252555106,-0.9212720434835953,0.1242806741969112,-0.5983960315181748,-0.18494335623235414,-0.8183808561975836,0.8513600414406984,1.717'\n",
            "b'1.6312258686346301,0.3522616607867429,0.04080576110152256,-0.1408895163348976,-0.4632103899987006,-0.06751623819156843,-0.8277122355407183,0.5966931783531273,3.376'\n",
            "b'-0.46794146200516895,-0.9293421252555106,0.11909925912590703,-0.060470113038678074,0.30344643606811583,-0.021851890609536125,1.873722084296329,-1.0411642940532422,1.012'\n",
            "b'-0.7432054083470616,0.9129633171802288,-0.644320243857189,-0.1479096959813185,0.7398510909061499,0.11427691039226903,-0.7950524078397521,0.6815821327156534,1.438'\n",
            "b'0.21174628471128154,1.1532640270631513,-0.2507761334605016,-0.2564987121705146,-0.6473894854916754,0.017590216427099285,0.7959477701644521,-1.1510205879341566,1.935'\n",
            "b'0.18702261628258646,-0.20843999560674303,0.005869659830725365,-0.2645340092721605,-0.011381870020860852,-0.015878889894211247,0.05876880205693385,0.17224840654049697,0.84'\n",
            "b'0.04049225382803661,-0.6890414153725881,-0.44379851741607473,0.022374585146687852,-0.22187226486997497,-0.1482850314959248,-0.8883662012710817,0.6366409215825501,2.852'\n",
            "b'-0.060214068004363165,0.7527628439249472,0.0835940301935345,-0.06250122441959183,-0.03497131082291674,-0.026442380178345683,1.0712234607868782,-1.3707331756959855,1.651'\n",
            "b'-0.8246762898717912,-0.04823952235146133,-0.3448658166118309,-0.08477587145199328,0.5012348243315076,-0.034699996532417135,0.5300034588851571,-0.08741192445075467,0.717'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1OkJLDLWWpW",
        "colab_type": "code",
        "outputId": "cff9a10f-2358-4d13-e691-ca95bee6e1e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# tf.io.decode_csv(str, record_defaults)\n",
        "\n",
        "sample_str = '1, 2, 3, 4, 5'\n",
        "record_defaults = [tf.constant(0, dtype=tf.int32),\n",
        "                   0,\n",
        "                   np.nan,\n",
        "                   \"hello\",\n",
        "                   tf.constant([])\n",
        "]\n",
        "parsed_fields = tf.io.decode_csv(sample_str, record_defaults)\n",
        "print(parsed_fields)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=124, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=125, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=126, shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: id=127, shape=(), dtype=string, numpy=b' 4'>, <tf.Tensor: id=128, shape=(), dtype=float32, numpy=5.0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUhyATIAXUNj",
        "colab_type": "code",
        "outputId": "a7f54b2b-5abb-4865-870f-9df018f26451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "    print(ex)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMy40nnIX4aY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15821b66-725c-4a58-d6f9-43a2f59a51d6"
      },
      "source": [
        "try:\n",
        "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "    print(ex)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya3x07pcY5Yb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "51636af0-b194-4257-f96b-08a5d5514e86"
      },
      "source": [
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults = defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "parse_csv_line(b'-0.4394346460367383,0.1920611875314612,-0.39172440230167493,-0.06233787211356993,0.682692061270399,-0.012080008421921133,0.935918460311448,-1.2458964781040367,1.618', \n",
        "               n_fields=9)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=213, shape=(8,), dtype=float32, numpy=\n",
              " array([-0.43943465,  0.19206119, -0.3917244 , -0.06233787,  0.68269205,\n",
              "        -0.01208001,  0.93591845, -1.2458965 ], dtype=float32)>,\n",
              " <tf.Tensor: id=214, shape=(1,), dtype=float32, numpy=array([1.618], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fycP-ocKURI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "d67b65e0-2018-4ef3-f911-afa71c1f956d"
      },
      "source": [
        "def csv_reader_dataset(filenames, n_readers=5,\n",
        "                       batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_csv_line, \n",
        "                          num_parallel_calls=n_parse_threads)\n",
        "    \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "train_set = csv_reader_dataset(train_filenames, batch_size=3)\n",
        "for x_batch, y_batch in train_set.take(2):\n",
        "    print(\"x:\")\n",
        "    pprint.pprint(x_batch)\n",
        "    print(\"y:\")\n",
        "    pprint.pprint(y_batch)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "<tf.Tensor: id=289, shape=(3, 8), dtype=float32, numpy=\n",
            "array([[ 0.63034356,  1.8741661 , -0.06713215, -0.12543367, -0.19737554,\n",
            "        -0.02272263, -0.69240725,  0.72652334],\n",
            "       [-0.82195884,  1.8741661 ,  0.1821235 , -0.03170019, -0.6011179 ,\n",
            "        -0.14337493,  1.0852206 , -0.8613995 ],\n",
            "       [ 0.4240821 ,  0.91296333, -0.04437482, -0.15297213, -0.24727628,\n",
            "        -0.10539167,  0.86126745, -1.335779  ]], dtype=float32)>\n",
            "y:\n",
            "<tf.Tensor: id=290, shape=(3, 1), dtype=float32, numpy=\n",
            "array([[2.419],\n",
            "       [1.054],\n",
            "       [3.955]], dtype=float32)>\n",
            "x:\n",
            "<tf.Tensor: id=293, shape=(3, 8), dtype=float32, numpy=\n",
            "array([[ 0.09734604,  0.75276285, -0.20218964, -0.19547   , -0.40605137,\n",
            "         0.00678553, -0.81371516,  0.6566148 ],\n",
            "       [ 0.04326301, -1.0895426 , -0.38878718, -0.10789865, -0.68186635,\n",
            "        -0.0723871 , -0.8883662 ,  0.8213992 ],\n",
            "       [ 1.6312258 ,  0.35226166,  0.04080576, -0.14088951, -0.4632104 ,\n",
            "        -0.06751624, -0.82771224,  0.59669316]], dtype=float32)>\n",
            "y:\n",
            "<tf.Tensor: id=294, shape=(3, 1), dtype=float32, numpy=\n",
            "array([[1.119],\n",
            "       [1.426],\n",
            "       [3.376]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PntXADOnV-Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames, batch_size = batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames, batch_size = batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRDDIPt4Wdjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1227
        },
        "outputId": "6ca79d4d-44a7-4fb2-ba1d-e9e914956d0e"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',\n",
        "                       input_shape=[8]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "            patience=5, min_delta=1e-2)]\n",
        "\n",
        "history = model.fit(train_set,\n",
        "                    validation_data = valid_set,\n",
        "                    steps_per_epoch = 11160 // batch_size, # train_set大小11160\n",
        "                    validation_steps = 3870 // batch_size, # valid_set大小3870\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 2.1260 - val_loss: 1.1943\n",
            "Epoch 2/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.9823 - val_loss: 0.9344\n",
            "Epoch 3/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8272 - val_loss: 0.8333\n",
            "Epoch 4/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7550 - val_loss: 0.7701\n",
            "Epoch 5/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6981 - val_loss: 0.7242\n",
            "Epoch 6/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6608 - val_loss: 0.6893\n",
            "Epoch 7/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6282 - val_loss: 0.6594\n",
            "Epoch 8/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6045 - val_loss: 0.6352\n",
            "Epoch 9/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5678 - val_loss: 0.6157\n",
            "Epoch 10/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5698 - val_loss: 0.5986\n",
            "Epoch 11/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5490 - val_loss: 0.5838\n",
            "Epoch 12/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5318 - val_loss: 0.5707\n",
            "Epoch 13/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5232 - val_loss: 0.5597\n",
            "Epoch 14/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5186 - val_loss: 0.5499\n",
            "Epoch 15/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5120 - val_loss: 0.5402\n",
            "Epoch 16/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4988 - val_loss: 0.5337\n",
            "Epoch 17/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4952 - val_loss: 0.5257\n",
            "Epoch 18/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4898 - val_loss: 0.5203\n",
            "Epoch 19/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4769 - val_loss: 0.5155\n",
            "Epoch 20/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4896 - val_loss: 0.5101\n",
            "Epoch 21/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4659 - val_loss: 0.5062\n",
            "Epoch 22/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4781 - val_loss: 0.5005\n",
            "Epoch 23/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4646 - val_loss: 0.4984\n",
            "Epoch 24/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4669 - val_loss: 0.4933\n",
            "Epoch 25/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4631 - val_loss: 0.4909\n",
            "Epoch 26/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4584 - val_loss: 0.4895\n",
            "Epoch 27/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4568 - val_loss: 0.4855\n",
            "Epoch 28/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4557 - val_loss: 0.4828\n",
            "Epoch 29/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4514 - val_loss: 0.4807\n",
            "Epoch 30/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4590 - val_loss: 0.4791\n",
            "Epoch 31/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4401 - val_loss: 0.4766\n",
            "Epoch 32/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4439 - val_loss: 0.4750\n",
            "Epoch 33/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4464 - val_loss: 0.4729\n",
            "Epoch 34/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4428 - val_loss: 0.4705\n",
            "Epoch 35/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4405 - val_loss: 0.4692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z9We7AoXiDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cf5cf33a-b19d-4ce9-8f7e-2b61ed56459d"
      },
      "source": [
        "model.evaluate(test_set, steps = 5160 // batch_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161/161 [==============================] - 0s 1ms/step - loss: 0.4575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4575354126947267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}