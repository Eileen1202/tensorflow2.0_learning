{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_data_generate_tfrecord.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvshyer/tensorflow2.0_learning/blob/master/tf_data_generate_tfrecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45KpLhbcNmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "7566a9d2-26ec-4663-909f-c635d77bde9a"
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "  Using cached https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.16.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.0.0a0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2CDf3iqZuzB",
        "colab_type": "code",
        "outputId": "4901919c-0714-47d9-8510-689694cddddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n",
            "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
            "matplotlib 3.0.3\n",
            "numpy 1.16.4\n",
            "pandas 0.24.2\n",
            "sklearn 0.21.2\n",
            "tensorflow 2.0.0-alpha0\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U4hG4EFcRDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "43a211e0-7307-45fe-d2b8-900eb997a66b"
      },
      "source": [
        "source_dir = \"./generate_csv/\"\n",
        "\n",
        "def get_filenames_by_prefix(source_dir, prefix_name):\n",
        "    all_files = os.listdir(source_dir)\n",
        "    results = []\n",
        "    for filename in all_files:\n",
        "        if filename.startswith(prefix_name):\n",
        "            results.append(os.path.join(source_dir, filename))\n",
        "    return results\n",
        "\n",
        "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
        "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
        "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(train_filenames)\n",
        "pprint.pprint(valid_filenames)\n",
        "pprint.pprint(test_filenames)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./generate_csv/train_15.csv',\n",
            " './generate_csv/train_18.csv',\n",
            " './generate_csv/train_09.csv',\n",
            " './generate_csv/train_10.csv',\n",
            " './generate_csv/train_06.csv',\n",
            " './generate_csv/train_16.csv',\n",
            " './generate_csv/train_02.csv',\n",
            " './generate_csv/train_12.csv',\n",
            " './generate_csv/train_17.csv',\n",
            " './generate_csv/train_01.csv',\n",
            " './generate_csv/train_13.csv',\n",
            " './generate_csv/train_08.csv',\n",
            " './generate_csv/train_00.csv',\n",
            " './generate_csv/train_05.csv',\n",
            " './generate_csv/train_11.csv',\n",
            " './generate_csv/train_04.csv',\n",
            " './generate_csv/train_14.csv',\n",
            " './generate_csv/train_07.csv',\n",
            " './generate_csv/train_03.csv',\n",
            " './generate_csv/train_19.csv']\n",
            "['./generate_csv/valid_01.csv',\n",
            " './generate_csv/valid_09.csv',\n",
            " './generate_csv/valid_06.csv',\n",
            " './generate_csv/valid_05.csv',\n",
            " './generate_csv/valid_07.csv',\n",
            " './generate_csv/valid_03.csv',\n",
            " './generate_csv/valid_04.csv',\n",
            " './generate_csv/valid_00.csv',\n",
            " './generate_csv/valid_02.csv',\n",
            " './generate_csv/valid_08.csv']\n",
            "['./generate_csv/test_05.csv',\n",
            " './generate_csv/test_08.csv',\n",
            " './generate_csv/test_01.csv',\n",
            " './generate_csv/test_02.csv',\n",
            " './generate_csv/test_03.csv',\n",
            " './generate_csv/test_09.csv',\n",
            " './generate_csv/test_04.csv',\n",
            " './generate_csv/test_00.csv',\n",
            " './generate_csv/test_07.csv',\n",
            " './generate_csv/test_06.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0K22Fwcd6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 读取CSV文件\n",
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults = defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filenames, n_readers=5,\n",
        "                       batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    \n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_csv_line,\n",
        "                          num_parallel_calls = n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,\n",
        "                               batch_size=batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,\n",
        "                               batch_size=batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,\n",
        "                               batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry4SnuyAeYFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serialize_example(x, y):\n",
        "    \"\"\" Convert x, y to tf.train.Example and serialize \"\"\"\n",
        "    input_features = tf.train.FloatList(value = x)\n",
        "    label = tf.train.FloatList(value = y)\n",
        "    features = tf.train.Features(\n",
        "        feature = {\n",
        "            \"input_features\": tf.train.Feature(float_list = input_features),\n",
        "            \"label\": tf.train.Feature(float_list = label)\n",
        "        }\n",
        "    )\n",
        "    example = tf.train.Example(features = features)\n",
        "    return example.SerializeToString()\n",
        "\n",
        "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
        "                             n_shards, steps_per_shard,\n",
        "                             compression_type = None):\n",
        "    options = tf.io.TFRecordOptions(\n",
        "        compression_type = compression_type)\n",
        "    all_filenames = []\n",
        "    for shard_id in range(n_shards):\n",
        "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
        "            base_filename, shard_id, n_shards)\n",
        "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
        "            for x_batch, y_batch in dataset.take(steps_per_shard):\n",
        "                for x_example, y_example in zip(x_batch, y_batch):\n",
        "                    writer.write(\n",
        "                        serialize_example(x_example, y_example))\n",
        "        all_filenames.append(filename_fullpath)\n",
        "    return all_filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpF0I0tLmilv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 不压缩版本\n",
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards \n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    \n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, \n",
        "    train_steps_per_shard, None)\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, \n",
        "    valid_steps_per_shard, None)\n",
        "test_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, \n",
        "    test_steps_per_shard, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3J18xrSnhm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 压缩版本\n",
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards \n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords_zip\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    \n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, \n",
        "    train_steps_per_shard, compression_type=\"GZIP\")\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, \n",
        "    valid_steps_per_shard, compression_type=\"GZIP\")\n",
        "test_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, \n",
        "    test_steps_per_shard, compression_type=\"GZIP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObZbOwtqLlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        },
        "outputId": "beb21d2d-0f7d-4582-df13-1e17db6d7a88"
      },
      "source": [
        "pprint.pprint(train_tfrecord_filenames)\n",
        "pprint.pprint(valid_tfrecord_filenames)\n",
        "pprint.pprint(test_tfrecord_filenames)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['generate_tfrecords_zip/train_00000-of-00020',\n",
            " 'generate_tfrecords_zip/train_00001-of-00020',\n",
            " 'generate_tfrecords_zip/train_00002-of-00020',\n",
            " 'generate_tfrecords_zip/train_00003-of-00020',\n",
            " 'generate_tfrecords_zip/train_00004-of-00020',\n",
            " 'generate_tfrecords_zip/train_00005-of-00020',\n",
            " 'generate_tfrecords_zip/train_00006-of-00020',\n",
            " 'generate_tfrecords_zip/train_00007-of-00020',\n",
            " 'generate_tfrecords_zip/train_00008-of-00020',\n",
            " 'generate_tfrecords_zip/train_00009-of-00020',\n",
            " 'generate_tfrecords_zip/train_00010-of-00020',\n",
            " 'generate_tfrecords_zip/train_00011-of-00020',\n",
            " 'generate_tfrecords_zip/train_00012-of-00020',\n",
            " 'generate_tfrecords_zip/train_00013-of-00020',\n",
            " 'generate_tfrecords_zip/train_00014-of-00020',\n",
            " 'generate_tfrecords_zip/train_00015-of-00020',\n",
            " 'generate_tfrecords_zip/train_00016-of-00020',\n",
            " 'generate_tfrecords_zip/train_00017-of-00020',\n",
            " 'generate_tfrecords_zip/train_00018-of-00020',\n",
            " 'generate_tfrecords_zip/train_00019-of-00020']\n",
            "['generate_tfrecords_zip/valid_00000-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00001-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00002-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00003-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00004-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00005-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00006-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00007-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00008-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00009-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00010-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00011-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00012-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00013-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00014-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00015-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00016-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00017-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00018-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00019-of-00020']\n",
            "['generate_tfrecords_zip/test_00000-of-00020',\n",
            " 'generate_tfrecords_zip/test_00001-of-00020',\n",
            " 'generate_tfrecords_zip/test_00002-of-00020',\n",
            " 'generate_tfrecords_zip/test_00003-of-00020',\n",
            " 'generate_tfrecords_zip/test_00004-of-00020',\n",
            " 'generate_tfrecords_zip/test_00005-of-00020',\n",
            " 'generate_tfrecords_zip/test_00006-of-00020',\n",
            " 'generate_tfrecords_zip/test_00007-of-00020',\n",
            " 'generate_tfrecords_zip/test_00008-of-00020',\n",
            " 'generate_tfrecords_zip/test_00009-of-00020',\n",
            " 'generate_tfrecords_zip/test_00010-of-00020',\n",
            " 'generate_tfrecords_zip/test_00011-of-00020',\n",
            " 'generate_tfrecords_zip/test_00012-of-00020',\n",
            " 'generate_tfrecords_zip/test_00013-of-00020',\n",
            " 'generate_tfrecords_zip/test_00014-of-00020',\n",
            " 'generate_tfrecords_zip/test_00015-of-00020',\n",
            " 'generate_tfrecords_zip/test_00016-of-00020',\n",
            " 'generate_tfrecords_zip/test_00017-of-00020',\n",
            " 'generate_tfrecords_zip/test_00018-of-00020',\n",
            " 'generate_tfrecords_zip/test_00019-of-00020']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enDEPeVwqnCD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "59a891e4-2c30-4a6b-85eb-5a10603f2cf9"
      },
      "source": [
        "expected_features = {\n",
        "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
        "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
        "}\n",
        "\n",
        "def parse_example(serialized_example):\n",
        "    example = tf.io.parse_single_example(\n",
        "        serialized_example, expected_features)\n",
        "    return example[\"input_features\"], example[\"label\"]\n",
        "\n",
        "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
        "                             batch_size=32, n_parse_threads=5,\n",
        "                             shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TFRecordDataset(\n",
        "            filename, compression_type=\"GZIP\"),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    \n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_example,\n",
        "                          num_parallel_calls = n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "tfrecords_train = tfrecords_reader_dataset(\n",
        "                    train_tfrecord_filenames, batch_size =3)\n",
        "for x_batch, y_batch in tfrecords_train.take(2):\n",
        "    print(x_batch)\n",
        "    print(y_batch)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.66722274 -0.04823952  0.34529406  0.53826684  1.8521839  -0.06112538\n",
            "  -0.8417093   1.5204847 ]\n",
            " [-0.66722274 -0.04823952  0.34529406  0.53826684  1.8521839  -0.06112538\n",
            "  -0.8417093   1.5204847 ]\n",
            " [-0.66722274 -0.04823952  0.34529406  0.53826684  1.8521839  -0.06112538\n",
            "  -0.8417093   1.5204847 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1.59]\n",
            " [1.59]\n",
            " [1.59]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.66722274 -0.04823952  0.34529406  0.53826684  1.8521839  -0.06112538\n",
            "  -0.8417093   1.5204847 ]\n",
            " [-0.66722274 -0.04823952  0.34529406  0.53826684  1.8521839  -0.06112538\n",
            "  -0.8417093   1.5204847 ]\n",
            " [ 0.8015443   0.27216142 -0.11624393 -0.20231152 -0.5430516  -0.02103962\n",
            "  -0.5897621  -0.08241846]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1.59 ]\n",
            " [1.59 ]\n",
            " [3.226]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08h3nbU8wtVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "tfrecords_train_set = tfrecords_reader_dataset(\n",
        "    train_tfrecord_filenames, batch_size=batch_size)\n",
        "tfrecords_valid_set = tfrecords_reader_dataset(\n",
        "    valid_tfrecord_filenames, batch_size=batch_size)\n",
        "tfrecords_test_set = tfrecords_reader_dataset(\n",
        "    test_tfrecord_filenames, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40BYXFa5xHnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1567
        },
        "outputId": "959c4670-91d2-40de-91ea-215ccd111907"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',\n",
        "                       input_shape=[8]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "            patience=5, min_delta=1e-2)]\n",
        "\n",
        "history = model.fit(tfrecords_train_set,\n",
        "                    validation_data = tfrecords_valid_set,\n",
        "                    steps_per_epoch = 11160 // batch_size, # train_set大小11160\n",
        "                    validation_steps = 3870 // batch_size, # valid_set大小3870\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 2.1283 - val_loss: 1.1530\n",
            "Epoch 2/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8896 - val_loss: 1.0542\n",
            "Epoch 3/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7710 - val_loss: 1.0112\n",
            "Epoch 4/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6943 - val_loss: 0.9661\n",
            "Epoch 5/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6533 - val_loss: 0.9229\n",
            "Epoch 6/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6140 - val_loss: 0.8805\n",
            "Epoch 7/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5816 - val_loss: 0.8481\n",
            "Epoch 8/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5491 - val_loss: 0.8110\n",
            "Epoch 9/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5283 - val_loss: 0.7806\n",
            "Epoch 10/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5039 - val_loss: 0.7540\n",
            "Epoch 11/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4823 - val_loss: 0.7345\n",
            "Epoch 12/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4652 - val_loss: 0.7174\n",
            "Epoch 13/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4511 - val_loss: 0.7037\n",
            "Epoch 14/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4486 - val_loss: 0.6890\n",
            "Epoch 15/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4413 - val_loss: 0.6745\n",
            "Epoch 16/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4299 - val_loss: 0.6631\n",
            "Epoch 17/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4219 - val_loss: 0.6494\n",
            "Epoch 18/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4131 - val_loss: 0.6406\n",
            "Epoch 19/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4006 - val_loss: 0.6280\n",
            "Epoch 20/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4002 - val_loss: 0.6212\n",
            "Epoch 21/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3938 - val_loss: 0.6109\n",
            "Epoch 22/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.6070\n",
            "Epoch 23/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3863 - val_loss: 0.5964\n",
            "Epoch 24/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3856 - val_loss: 0.5892\n",
            "Epoch 25/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3769 - val_loss: 0.5803\n",
            "Epoch 26/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3723 - val_loss: 0.5758\n",
            "Epoch 27/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3662 - val_loss: 0.5712\n",
            "Epoch 28/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3623 - val_loss: 0.5669\n",
            "Epoch 29/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3649 - val_loss: 0.5621\n",
            "Epoch 30/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3643 - val_loss: 0.5563\n",
            "Epoch 31/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3590 - val_loss: 0.5540\n",
            "Epoch 32/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3558 - val_loss: 0.5483\n",
            "Epoch 33/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3528 - val_loss: 0.5460\n",
            "Epoch 34/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3448 - val_loss: 0.5397\n",
            "Epoch 35/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3476 - val_loss: 0.5390\n",
            "Epoch 36/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3433 - val_loss: 0.5350\n",
            "Epoch 37/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3476 - val_loss: 0.5357\n",
            "Epoch 38/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3414 - val_loss: 0.5302\n",
            "Epoch 39/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3434 - val_loss: 0.5270\n",
            "Epoch 40/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3376 - val_loss: 0.5224\n",
            "Epoch 41/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3355 - val_loss: 0.5207\n",
            "Epoch 42/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3312 - val_loss: 0.5192\n",
            "Epoch 43/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3296 - val_loss: 0.5191\n",
            "Epoch 44/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3322 - val_loss: 0.5174\n",
            "Epoch 45/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3327 - val_loss: 0.5137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFqoeDuDx3ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "eec2afa2-3c18-4599-9c7e-7146b922df0c"
      },
      "source": [
        "model.evaluate(tfrecords_test_set, steps = 5160 // batch_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161/161 [==============================] - 0s 1ms/step - loss: 0.3966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3966156949537881"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EouGl2SDx6Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}