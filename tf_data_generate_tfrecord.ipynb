{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_data_generate_tfrecord.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvshyer/tensorflow2.0_learning/blob/master/tf_data_generate_tfrecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45KpLhbcNmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "7566a9d2-26ec-4663-909f-c635d77bde9a"
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "  Using cached https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.16.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.0.0a0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2CDf3iqZuzB",
        "colab_type": "code",
        "outputId": "4901919c-0714-47d9-8510-689694cddddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n",
            "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
            "matplotlib 3.0.3\n",
            "numpy 1.16.4\n",
            "pandas 0.24.2\n",
            "sklearn 0.21.2\n",
            "tensorflow 2.0.0-alpha0\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U4hG4EFcRDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "43a211e0-7307-45fe-d2b8-900eb997a66b"
      },
      "source": [
        "source_dir = \"./generate_csv/\"\n",
        "\n",
        "def get_filenames_by_prefix(source_dir, prefix_name):\n",
        "    all_files = os.listdir(source_dir)\n",
        "    results = []\n",
        "    for filename in all_files:\n",
        "        if filename.startswith(prefix_name):\n",
        "            results.append(os.path.join(source_dir, filename))\n",
        "    return results\n",
        "\n",
        "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
        "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
        "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(train_filenames)\n",
        "pprint.pprint(valid_filenames)\n",
        "pprint.pprint(test_filenames)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./generate_csv/train_15.csv',\n",
            " './generate_csv/train_18.csv',\n",
            " './generate_csv/train_09.csv',\n",
            " './generate_csv/train_10.csv',\n",
            " './generate_csv/train_06.csv',\n",
            " './generate_csv/train_16.csv',\n",
            " './generate_csv/train_02.csv',\n",
            " './generate_csv/train_12.csv',\n",
            " './generate_csv/train_17.csv',\n",
            " './generate_csv/train_01.csv',\n",
            " './generate_csv/train_13.csv',\n",
            " './generate_csv/train_08.csv',\n",
            " './generate_csv/train_00.csv',\n",
            " './generate_csv/train_05.csv',\n",
            " './generate_csv/train_11.csv',\n",
            " './generate_csv/train_04.csv',\n",
            " './generate_csv/train_14.csv',\n",
            " './generate_csv/train_07.csv',\n",
            " './generate_csv/train_03.csv',\n",
            " './generate_csv/train_19.csv']\n",
            "['./generate_csv/valid_01.csv',\n",
            " './generate_csv/valid_09.csv',\n",
            " './generate_csv/valid_06.csv',\n",
            " './generate_csv/valid_05.csv',\n",
            " './generate_csv/valid_07.csv',\n",
            " './generate_csv/valid_03.csv',\n",
            " './generate_csv/valid_04.csv',\n",
            " './generate_csv/valid_00.csv',\n",
            " './generate_csv/valid_02.csv',\n",
            " './generate_csv/valid_08.csv']\n",
            "['./generate_csv/test_05.csv',\n",
            " './generate_csv/test_08.csv',\n",
            " './generate_csv/test_01.csv',\n",
            " './generate_csv/test_02.csv',\n",
            " './generate_csv/test_03.csv',\n",
            " './generate_csv/test_09.csv',\n",
            " './generate_csv/test_04.csv',\n",
            " './generate_csv/test_00.csv',\n",
            " './generate_csv/test_07.csv',\n",
            " './generate_csv/test_06.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0K22Fwcd6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 读取CSV文件\n",
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults = defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filenames, n_readers=5,\n",
        "                       batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    \n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_csv_line,\n",
        "                          num_parallel_calls = n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,\n",
        "                               batch_size=batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,\n",
        "                               batch_size=batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,\n",
        "                               batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry4SnuyAeYFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serialize_example(x, y):\n",
        "    \"\"\" Convert x, y to tf.train.Example and serialize \"\"\"\n",
        "    input_features = tf.train.FloatList(value = x)\n",
        "    label = tf.train.FloatList(value = y)\n",
        "    features = tf.train.Features(\n",
        "        feature = {\n",
        "            \"input_features\": tf.train.Feature(float_list = input_features),\n",
        "            \"label\": tf.train.Feature(float_list = label)\n",
        "        }\n",
        "    )\n",
        "    example = tf.train.Example(features = features)\n",
        "    return example.SerializeToString()\n",
        "\n",
        "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
        "                             n_shards, steps_per_shard,\n",
        "                             compression_type = None):\n",
        "    options = tf.io.TFRecordOptions(\n",
        "        compression_type = compression_type)\n",
        "    all_filenames = []\n",
        "    for shard_id in range(n_shards):\n",
        "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
        "            base_filename, shard_id, n_shards)\n",
        "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
        "            for x_batch, y_batch in dataset.take(steps_per_shard):\n",
        "                for x_example, y_example in zip(x_batch, y_batch):\n",
        "                    writer.write(\n",
        "                        serialize_example(x_example, y_example))\n",
        "        all_filenames.append(filename_fullpath)\n",
        "    return all_filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpF0I0tLmilv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 不压缩版本\n",
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards \n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    \n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, \n",
        "    train_steps_per_shard, None)\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, \n",
        "    valid_steps_per_shard, None)\n",
        "test_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, \n",
        "    test_steps_per_shard, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3J18xrSnhm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 压缩版本\n",
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards \n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords_zip\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    \n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, \n",
        "    train_steps_per_shard, compression_type=\"GZIP\")\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, \n",
        "    valid_steps_per_shard, compression_type=\"GZIP\")\n",
        "test_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, \n",
        "    test_steps_per_shard, compression_type=\"GZIP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObZbOwtqLlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}